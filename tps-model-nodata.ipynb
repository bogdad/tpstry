{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c605334-5264-47ed-a204-38523ace182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Familiar imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For ordinal encoding categorical variables, splitting data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "# For training random forest model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler,  RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from numba import cuda\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fe4aab-1c43-4859-839d-69b1533023d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, regularizers\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from pandas.util import hash_pandas_object\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b29fcf4-a5fe-495c-b02a-9df7c10679f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1341577361757076398\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9898950656\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6604092024446162092\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2a:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eab628b-9547-442b-9024-15b27b3a812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01470e4-069e-40d1-84bd-15aba1838dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "rawtrain = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\", index_col=0)\n",
    "rawtest = pd.read_csv(\"../input/tabular-playground-series-nov-2021/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa3a0697-bf43-4dd4-917b-526f16834e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600000 entries, 0 to 599999\n",
      "Columns: 101 entries, f0 to target\n",
      "dtypes: float64(100), int64(1)\n",
      "memory usage: 466.9 MB\n"
     ]
    }
   ],
   "source": [
    "rawtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df515fb-c673-4467-92a3-c7b8a219e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import annoy\n",
    "ann_fname = \"./models/annoy1.npy\"\n",
    "ann_train = rawtrain.copy() \n",
    "transformer = StandardScaler().fit(ann_train)\n",
    "ann_train = transformer.transform(ann_train)\n",
    "ann_train = pd.DataFrame(data=ann_train, index=rawtrain.index, columns=rawtrain.columns)\n",
    "ann_train.drop([\"target\"], axis=1, inplace=True)\n",
    "if os.path.exists(ann_fname):\n",
    "    f = 100\n",
    "    ann_idx = annoy.AnnoyIndex(f, 'angular')\n",
    "    ann_idx.load(ann_fname)\n",
    "else:   \n",
    "    f = 100\n",
    "    ann_idx = annoy.AnnoyIndex(f, 'angular')\n",
    "    for index, row in ann_train.iterrows():\n",
    "        ann_idx.add_item(index, row)\n",
    "    ann_idx.build(64)\n",
    "    ann_idx.save(ann_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89007b9-05a4-46b1-bb27-55b73c0a989c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>6.000000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.486900e-16</td>\n",
       "      <td>5.498653e-16</td>\n",
       "      <td>-1.848595e-16</td>\n",
       "      <td>-1.231607e-17</td>\n",
       "      <td>-2.024573e-16</td>\n",
       "      <td>-6.557836e-16</td>\n",
       "      <td>5.020221e-16</td>\n",
       "      <td>-3.015543e-16</td>\n",
       "      <td>4.177991e-16</td>\n",
       "      <td>1.216449e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.594932e-16</td>\n",
       "      <td>3.210706e-16</td>\n",
       "      <td>-7.114901e-17</td>\n",
       "      <td>2.028600e-17</td>\n",
       "      <td>-2.368476e-18</td>\n",
       "      <td>1.450928e-16</td>\n",
       "      <td>3.491133e-17</td>\n",
       "      <td>-5.476863e-16</td>\n",
       "      <td>-2.664535e-17</td>\n",
       "      <td>-8.360720e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.855227e+00</td>\n",
       "      <td>-2.394794e+00</td>\n",
       "      <td>-3.895243e+00</td>\n",
       "      <td>-2.600446e+00</td>\n",
       "      <td>-8.105763e+00</td>\n",
       "      <td>-2.384985e+00</td>\n",
       "      <td>-2.403430e+00</td>\n",
       "      <td>-2.487197e+00</td>\n",
       "      <td>-2.465967e+00</td>\n",
       "      <td>-6.428692e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.327214e+01</td>\n",
       "      <td>-2.374167e+00</td>\n",
       "      <td>-1.808369e+01</td>\n",
       "      <td>-3.956981e+01</td>\n",
       "      <td>-7.026692e+00</td>\n",
       "      <td>-6.612998e+00</td>\n",
       "      <td>-2.651774e+00</td>\n",
       "      <td>-2.521090e+00</td>\n",
       "      <td>-9.519664e+00</td>\n",
       "      <td>-1.097197e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.364845e-01</td>\n",
       "      <td>-8.438471e-01</td>\n",
       "      <td>-4.767998e-01</td>\n",
       "      <td>-7.807393e-01</td>\n",
       "      <td>-3.787915e-01</td>\n",
       "      <td>-8.292945e-01</td>\n",
       "      <td>-8.400459e-01</td>\n",
       "      <td>-7.988755e-01</td>\n",
       "      <td>-8.128648e-01</td>\n",
       "      <td>-2.721238e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.505446e-01</td>\n",
       "      <td>-7.975928e-01</td>\n",
       "      <td>-2.486452e-01</td>\n",
       "      <td>-2.924307e-01</td>\n",
       "      <td>-2.767824e-01</td>\n",
       "      <td>-3.871222e-01</td>\n",
       "      <td>-8.341527e-01</td>\n",
       "      <td>-8.144316e-01</td>\n",
       "      <td>-3.332156e-01</td>\n",
       "      <td>-3.961681e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.995015e-01</td>\n",
       "      <td>1.216877e-02</td>\n",
       "      <td>-3.135851e-01</td>\n",
       "      <td>-8.916106e-03</td>\n",
       "      <td>-2.784834e-01</td>\n",
       "      <td>2.149922e-02</td>\n",
       "      <td>6.496472e-02</td>\n",
       "      <td>-8.783022e-03</td>\n",
       "      <td>-4.070035e-02</td>\n",
       "      <td>-1.792356e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.482936e-01</td>\n",
       "      <td>-3.735838e-02</td>\n",
       "      <td>-1.574661e-01</td>\n",
       "      <td>-2.309440e-02</td>\n",
       "      <td>-1.763875e-01</td>\n",
       "      <td>-2.116787e-01</td>\n",
       "      <td>3.470647e-02</td>\n",
       "      <td>6.027919e-02</td>\n",
       "      <td>-2.311456e-01</td>\n",
       "      <td>-2.437825e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.735590e-01</td>\n",
       "      <td>8.301332e-01</td>\n",
       "      <td>-7.942616e-03</td>\n",
       "      <td>8.156141e-01</td>\n",
       "      <td>-1.560231e-01</td>\n",
       "      <td>8.043581e-01</td>\n",
       "      <td>8.294525e-01</td>\n",
       "      <td>8.079834e-01</td>\n",
       "      <td>8.258958e-01</td>\n",
       "      <td>-7.898759e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.807914e-01</td>\n",
       "      <td>8.099809e-01</td>\n",
       "      <td>-5.487777e-02</td>\n",
       "      <td>2.407781e-01</td>\n",
       "      <td>-6.886846e-02</td>\n",
       "      <td>-2.105475e-02</td>\n",
       "      <td>8.167297e-01</td>\n",
       "      <td>8.192050e-01</td>\n",
       "      <td>-1.104176e-01</td>\n",
       "      <td>-6.861488e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.622165e+01</td>\n",
       "      <td>2.399673e+00</td>\n",
       "      <td>1.053503e+01</td>\n",
       "      <td>2.507723e+00</td>\n",
       "      <td>1.937210e+01</td>\n",
       "      <td>2.533230e+00</td>\n",
       "      <td>2.485380e+00</td>\n",
       "      <td>2.379807e+00</td>\n",
       "      <td>2.512349e+00</td>\n",
       "      <td>1.645942e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.691483e+01</td>\n",
       "      <td>2.677082e+00</td>\n",
       "      <td>3.329260e+01</td>\n",
       "      <td>8.501020e+01</td>\n",
       "      <td>1.835081e+01</td>\n",
       "      <td>1.697913e+01</td>\n",
       "      <td>2.378367e+00</td>\n",
       "      <td>2.331103e+00</td>\n",
       "      <td>2.431996e+01</td>\n",
       "      <td>2.214102e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 f0            f1            f2            f3            f4  \\\n",
       "count  6.000000e+05  6.000000e+05  6.000000e+05  6.000000e+05  6.000000e+05   \n",
       "mean  -2.486900e-16  5.498653e-16 -1.848595e-16 -1.231607e-17 -2.024573e-16   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -7.855227e+00 -2.394794e+00 -3.895243e+00 -2.600446e+00 -8.105763e+00   \n",
       "25%   -5.364845e-01 -8.438471e-01 -4.767998e-01 -7.807393e-01 -3.787915e-01   \n",
       "50%   -3.995015e-01  1.216877e-02 -3.135851e-01 -8.916106e-03 -2.784834e-01   \n",
       "75%    1.735590e-01  8.301332e-01 -7.942616e-03  8.156141e-01 -1.560231e-01   \n",
       "max    1.622165e+01  2.399673e+00  1.053503e+01  2.507723e+00  1.937210e+01   \n",
       "\n",
       "                 f5            f6            f7            f8            f9  \\\n",
       "count  6.000000e+05  6.000000e+05  6.000000e+05  6.000000e+05  6.000000e+05   \n",
       "mean  -6.557836e-16  5.020221e-16 -3.015543e-16  4.177991e-16  1.216449e-16   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -2.384985e+00 -2.403430e+00 -2.487197e+00 -2.465967e+00 -6.428692e+00   \n",
       "25%   -8.292945e-01 -8.400459e-01 -7.988755e-01 -8.128648e-01 -2.721238e-01   \n",
       "50%    2.149922e-02  6.496472e-02 -8.783022e-03 -4.070035e-02 -1.792356e-01   \n",
       "75%    8.043581e-01  8.294525e-01  8.079834e-01  8.258958e-01 -7.898759e-02   \n",
       "max    2.533230e+00  2.485380e+00  2.379807e+00  2.512349e+00  1.645942e+01   \n",
       "\n",
       "       ...           f90           f91           f92           f93  \\\n",
       "count  ...  6.000000e+05  6.000000e+05  6.000000e+05  6.000000e+05   \n",
       "mean   ...  1.594932e-16  3.210706e-16 -7.114901e-17  2.028600e-17   \n",
       "std    ...  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min    ... -3.327214e+01 -2.374167e+00 -1.808369e+01 -3.956981e+01   \n",
       "25%    ... -4.505446e-01 -7.975928e-01 -2.486452e-01 -2.924307e-01   \n",
       "50%    ... -1.482936e-01 -3.735838e-02 -1.574661e-01 -2.309440e-02   \n",
       "75%    ...  1.807914e-01  8.099809e-01 -5.487777e-02  2.407781e-01   \n",
       "max    ...  5.691483e+01  2.677082e+00  3.329260e+01  8.501020e+01   \n",
       "\n",
       "                f94           f95           f96           f97           f98  \\\n",
       "count  6.000000e+05  6.000000e+05  6.000000e+05  6.000000e+05  6.000000e+05   \n",
       "mean  -2.368476e-18  1.450928e-16  3.491133e-17 -5.476863e-16 -2.664535e-17   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -7.026692e+00 -6.612998e+00 -2.651774e+00 -2.521090e+00 -9.519664e+00   \n",
       "25%   -2.767824e-01 -3.871222e-01 -8.341527e-01 -8.144316e-01 -3.332156e-01   \n",
       "50%   -1.763875e-01 -2.116787e-01  3.470647e-02  6.027919e-02 -2.311456e-01   \n",
       "75%   -6.886846e-02 -2.105475e-02  8.167297e-01  8.192050e-01 -1.104176e-01   \n",
       "max    1.835081e+01  1.697913e+01  2.378367e+00  2.331103e+00  2.431996e+01   \n",
       "\n",
       "                f99  \n",
       "count  6.000000e+05  \n",
       "mean  -8.360720e-17  \n",
       "std    1.000001e+00  \n",
       "min   -1.097197e+01  \n",
       "25%   -3.961681e-01  \n",
       "50%   -2.437825e-01  \n",
       "75%   -6.861488e-02  \n",
       "max    2.214102e+01  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1781827e-f77c-423c-8817-b677c64cf73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_train = rawtrain.copy()\n",
    "flip_test = rawtest.copy()\n",
    "flip_y = flip_train['target'].copy()\n",
    "flip_x = flip_train.drop(['target'], axis=1)\n",
    "\n",
    "ole_fname = \"./models/ole.npy\"\n",
    "if os.path.exists(ole_fname):\n",
    "    ordered_label_errors = np.load(ole_fname)\n",
    "else:\n",
    "    # Cross validate ridge\n",
    "    spl = 5\n",
    "    kf = StratifiedKFold(n_splits=spl, shuffle=True)\n",
    "    psx = np.zeros((flip_x.shape[0], 2))\n",
    "    mse = []\n",
    "    for train_idx, valid_idx in tqdm(kf.split(flip_x, flip_y)):\n",
    "        model = LogisticRegression(solver=\"liblinear\")\n",
    "        Xt = flip_x.iloc[train_idx, :]\n",
    "        yt = flip_y.iloc[train_idx]\n",
    "        Xv = flip_x.iloc[valid_idx, :]\n",
    "        yv = flip_y.iloc[valid_idx]\n",
    "        model.fit(Xt, yt)\n",
    "        y_pred = model.predict_proba(Xv)\n",
    "        psx[valid_idx, :] = y_pred\n",
    "        print(y_pred.shape)\n",
    "\n",
    "    from cleanlab.pruning import get_noise_indices\n",
    "    ordered_label_errors = get_noise_indices(\n",
    "        s=flip_y,\n",
    "        psx=psx,\n",
    "        sorted_index_method='normalized_margin', # Orders label errors\n",
    "    )\n",
    "    np.save(ole_fname, ordered_label_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67b1329-bff7-462b-bc21-3bf53b019930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(ordered_label_errors):\n",
    "    import annoy\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    rawtrain = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\", index_col=0)\n",
    "    flip_train = rawtrain.copy()\n",
    "    flip_y = flip_train['target'].copy()\n",
    "    f = 100\n",
    "    ann_fname = \"./models/annoy1.npy\"\n",
    "    ann_idx = annoy.AnnoyIndex(f, 'angular')\n",
    "    ann_idx.load(ann_fname)\n",
    "    bads = np.zeros(ordered_label_errors.shape)\n",
    "    for i, it in enumerate(ordered_label_errors):\n",
    "        yit = flip_y[it]\n",
    "        #print(yit)\n",
    "        neigh=ann_idx.get_nns_by_item(it, 20)\n",
    "        #inter = np.intersect1d(ordered_label_errors, neigh)\n",
    "        #print(inter)\n",
    "        norm_neigh = np.setdiff1d(neigh, ordered_label_errors)\n",
    "        #print(flip_y[norm_neigh])\n",
    "        #plt.hist(flip_y[norm_neigh])\n",
    "        n_true = (flip_y[norm_neigh] == flip_y[it]).sum()\n",
    "        n_false = (flip_y[norm_neigh] != flip_y[it]).sum()\n",
    "        ratio = n_false / (n_true + n_false)\n",
    "        bads[i]=ratio\n",
    "    return [ordered_label_errors, bads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa90b13-6ee4-468a-b693-2788851c2c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bads_fname=\"./models/bads.npy\"\n",
    "badsi_fname = \"./models/bads_i.npy\"\n",
    "if os.path.exists(bads_fname):\n",
    "    badsi = np.load(badsi_fname)\n",
    "    bads = np.load(bads_fname)\n",
    "else:\n",
    "    from pathos.multiprocessing import ProcessingPool as Pool\n",
    "    # checking errors with annoy\n",
    "    bads = []\n",
    "    badsi = []\n",
    "    with Pool(processes=10) as p:\n",
    "        for part in p.imap(count, np.array_split(ordered_label_errors, 60)):\n",
    "            badsi.append(part[0])\n",
    "            bads.append(part[1])\n",
    "    bads = np.concatenate(bads, axis=0)\n",
    "    badsi = np.concatenate(badsi, axis=0)\n",
    "    np.save(bads_fname, bads)\n",
    "    np.save(badsi_fname, badsi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4753d3b-ad76-4513-9b63-fda944da9982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         1.         ... 0.73684211 0.57894737 0.42105263]\n",
      "[415753 288596 165241 ...  95208 234563 454154]\n"
     ]
    }
   ],
   "source": [
    "print(bads)\n",
    "print(badsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e728851-a60e-47d0-b15d-684709c75270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  100.,  1126.,  6104., 17523., 34845., 23475., 38151., 22226.,\n",
       "         5924.,   527.]),\n",
       " array([0.05263158, 0.14736842, 0.24210526, 0.33684211, 0.43157895,\n",
       "        0.52631579, 0.62105263, 0.71578947, 0.81052632, 0.90526316,\n",
       "        1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(bads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c579b82-78a0-4dd6-8fce-ff754a90476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68742"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badsi[bads > 0.6].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef085024-8a4e-4384-92c3-94012b8dbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rawtrain.copy()\n",
    "prey = train['target'].copy()\n",
    "features = train.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b688a92-cb3f-4dbb-b020-a4521b5b9283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preX = features.copy()\n",
    "preX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e3ec0b1-b94e-4178-9aef-ed947899181c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([75169.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0., 74832.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(prey[ordered_label_errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd33868-c0bd-4c7d-a8eb-8cd14a2f435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df[df.score < 50].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da91226f-6670-4877-8647-10351740a507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = preX#.drop(ordered_label_errors)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3db09a8a-6a17-4331-a9af-8056ade3660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = prey#.drop(ordered_label_errors)\n",
    "#y[ordered_label_errors] = 1 - y[ordered_label_errors]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae6e6a75-1ccc-4390-8da2-0982867ba518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540000, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = rawtest.copy()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53d54f8c-7665-469a-9541-524e959c83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_parameters = {\n",
    "    \"device\":'gpu', \"metric\":\"auc\",\n",
    "  'learning_rate': 0.088202251342637, 'max_depth': 9, 'min_data_in_leaf': 622, 'n_estimators': 6675, 'num_leaves': 2, 'reg_alpha': 3.2551114904483622, 'reg_lambda': 4.3256459403144465\n",
    "}\n",
    "xgboost_parameters = { 'n_jobs':4, \n",
    "                       'tree_method': 'gpu_hist', \n",
    "                        'gpu_id': 0,\n",
    "'alpha': 6.585083548901454, 'gamma': 0.010815026816252349, 'lambda': 3.7699082658342555, 'learning_rate': 0.05935377531038842, 'max_depth': 1, 'min_child_weight': 44.52085850052923, 'n_estimators': 11303, 'subsample': 0.9220131617216115\n",
    "}\n",
    "\n",
    "catboost_parameters={\n",
    "    'task_type':\"GPU\", \"loss_function\":\"RMSE\",\n",
    "'bagging_temperature': 4.021848378181608, 'depth': 7, 'iterations': 9560, 'l2_leaf_reg': 98.16696275126748, 'learning_rate': 0.031190673582301456\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "739391a9-1891-42b6-8bbb-36a1e1fec7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModel:\n",
    "    def __init__(self, model, session):\n",
    "        self.model = model\n",
    "        self.session = session\n",
    "    def predict(self, test):\n",
    "        out = self.model.predict(test)\n",
    "        return np.reshape(out,(out.size,))\n",
    "    def predict_proba(self, test):\n",
    "        out = self.model.predict(test)\n",
    "        return np.reshape(out,(out.size,)) \n",
    "\n",
    "input_shape = [X.shape[1]]\n",
    "def create_keras():\n",
    "    model = keras.Sequential([\n",
    "        layers.BatchNormalization(input_shape = input_shape),\n",
    "        layers.GaussianNoise(stddev=0.2),\n",
    "        layers.Dense(units = 4096, activity_regularizer=regularizers.l1(10e-5)),\n",
    "        layers.LeakyReLU(alpha=0.05),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(rate = 0.3),\n",
    "        layers.Dense(units = 512),\n",
    "        layers.LeakyReLU(alpha=0.05),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(rate = 0.3),\n",
    "        layers.Dense(units = 4096),\n",
    "        layers.LeakyReLU(alpha=0.05),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(rate = 0.3),\n",
    "        layers.Dense(units = 1, activation = 'hard_sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    config = tf.compat.v1.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.allow_growth=True\n",
    "    session = tf.compat.v1.Session(config=config)\n",
    "    return KerasModel(model, session)\n",
    "\n",
    "def fit_keras(model, Xt, yt, Xv, yv):\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        min_delta=0.0001, # minimium amount of change to count as an improvement\n",
    "        patience=100, # how many epochs to wait before stopping\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    history = model.model.fit(Xt, yt, validation_data=(Xv, yv), \n",
    "    callbacks = [early_stopping, tensorboard_callback], \n",
    "        batch_size=6000, epochs=600, verbose=1)\n",
    "    del history\n",
    "def predict_keras(model, Xt):\n",
    "    return model.predict(Xt)\n",
    "def clear_keras(model):\n",
    "    gc.collect()\n",
    "    del model.model   \n",
    "    model.session.close()\n",
    "    del model.session\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "def shutdown_keras():\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a9dbf1a-c08c-482d-a363-258ac1eb60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_catboost(model, Xt, yt, Xv, yv):\n",
    "    model.fit(Xt, yt, early_stopping_rounds = 400, eval_set=[(Xv, yv)], verbose = 0)\n",
    "def fit_lgbm(model, Xt, yt, Xv, yv):\n",
    "    model.fit(Xt, yt, early_stopping_rounds = 399, eval_set=[(Xv, yv)], verbose = -1)  \n",
    "def fit_xgboost(model, Xt, yt, Xv, yv):\n",
    "    model.fit(Xt, yt, early_stopping_rounds = 400, eval_set=[(Xv, yv)], verbose=False)\n",
    "def predict_catboost(model, Xt):\n",
    "    return model.predict(Xt)\n",
    "def predict_lgbm(model, Xt):\n",
    "    return model.predict(Xt)\n",
    "def predict_xgboost(model, Xt):\n",
    "    return model.predict_proba(Xt)[:, 1]\n",
    "    \n",
    "model_types = [\n",
    "   {\"name\": \"keras\", \"create\": create_keras, \"fit\": fit_keras, \"predict\":predict_keras, \"clear\": clear_keras, \"shutdown\": shutdown_keras},\n",
    "   # {\"name\": \"lgbm\", \"create\": lambda: LGBMRegressor(**lgbm_parameters), \"fit\": fit_lgbm, \"predict\": predict_lgbm },\n",
    "   # {'name': 'xgboost', 'create': lambda: XGBClassifier(**xgboost_parameters, eval_metric=\"auc\", use_label_encoder=False), 'fit': fit_xgboost, 'predict': predict_xgboost},\n",
    "   # {\"name\": \"catboost\", \"create\": lambda: CatBoostRegressor(**catboost_parameters), \"fit\": fit_catboost, \"predict\": predict_catboost},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6445fda-0064-4431-88a9-ac8a9b8f7558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'keras',\n",
       "  'create': <function __main__.create_keras()>,\n",
       "  'fit': <function __main__.fit_keras(model, Xt, yt, Xv, yv)>,\n",
       "  'predict': <function __main__.predict_keras(model, Xt)>,\n",
       "  'clear': <function __main__.clear_keras(model)>,\n",
       "  'shutdown': <function __main__.shutdown_keras()>}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70c4d02-f126-481b-a322-168a1b3c1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hash = str(hash(hashlib.sha256(pd.util.hash_pandas_object(X, index=True).values).hexdigest()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "580ceb0c-c185-4571-a9a4-b5b0bc14166a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating splits\n"
     ]
    }
   ],
   "source": [
    "splits_file=\"./models/splits_\"+ str(input_hash) + \".npy\"\n",
    "if os.path.exists(splits_file):\n",
    "    print(\"splits from file\")\n",
    "    splits_arr = np.load(splits_file, allow_pickle=True)\n",
    "    splits = splits_arr[0]\n",
    "else:\n",
    "    print(\"generating splits\")\n",
    "    spl = 10\n",
    "    kf = StratifiedKFold(n_splits=spl, shuffle=True)\n",
    "    split_list=[]\n",
    "    for train_idx, valid_idx in kf.split(X,y.round()):\n",
    "        split_list.append({\"train\": train_idx.copy(), \"valid\": valid_idx.copy()})\n",
    "    splits = {}\n",
    "    splits[\"list\"] = split_list\n",
    "    hashes = []\n",
    "    hashes.append(hash(str(split_list)))\n",
    "    hashes.append(input_hash)\n",
    "    splits[\"hash\"] = hash(str(hashes))\n",
    "    splits[\"spl\"] = spl\n",
    "    np.save(splits_file, [splits])\n",
    "spl = splits['spl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb155e-3125-4392-aa57-a3a0258fb1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting keras\n",
      "fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                               | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2a:00.0, compute capability: 8.6\n",
      "\n",
      "Epoch 1/600\n",
      "90/90 [==============================] - 10s 80ms/step - loss: 5.1920 - auc: 0.6328 - val_loss: 5.2670 - val_auc: 0.6100\n",
      "Epoch 2/600\n",
      "90/90 [==============================] - 4s 48ms/step - loss: 6.7401 - auc: 0.5490 - val_loss: 6.7517 - val_auc: 0.5373\n",
      "Epoch 3/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 6.2334 - auc: 0.5820 - val_loss: 6.0506 - val_auc: 0.5918\n",
      "Epoch 4/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.7319 - auc: 0.6140 - val_loss: 5.5055 - val_auc: 0.6276\n",
      "Epoch 5/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.3149 - auc: 0.6414 - val_loss: 5.4338 - val_auc: 0.6342\n",
      "Epoch 6/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4698 - auc: 0.6318 - val_loss: 5.6146 - val_auc: 0.6226\n",
      "Epoch 7/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4308 - auc: 0.6338 - val_loss: 5.2083 - val_auc: 0.6491\n",
      "Epoch 8/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.1822 - auc: 0.6505 - val_loss: 5.0217 - val_auc: 0.6612\n",
      "Epoch 9/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.0938 - auc: 0.6569 - val_loss: 4.9872 - val_auc: 0.6652\n",
      "Epoch 10/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9484 - auc: 0.6671 - val_loss: 4.7348 - val_auc: 0.6815\n",
      "Epoch 11/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.8800 - auc: 0.6717 - val_loss: 4.9668 - val_auc: 0.6667\n",
      "Epoch 12/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.9832 - auc: 0.6634 - val_loss: 4.8534 - val_auc: 0.6705\n",
      "Epoch 13/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7966 - auc: 0.6745 - val_loss: 4.5464 - val_auc: 0.6922\n",
      "Epoch 14/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.6117 - auc: 0.6874 - val_loss: 4.5677 - val_auc: 0.6919\n",
      "Epoch 15/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.6897 - auc: 0.6822 - val_loss: 4.5944 - val_auc: 0.6894\n",
      "Epoch 16/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8176 - auc: 0.6735 - val_loss: 4.7396 - val_auc: 0.6803\n",
      "Epoch 17/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.6954 - auc: 0.6815 - val_loss: 4.7794 - val_auc: 0.6760\n",
      "Epoch 18/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8016 - auc: 0.6737 - val_loss: 4.6746 - val_auc: 0.6819\n",
      "Epoch 19/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8766 - auc: 0.6686 - val_loss: 4.7760 - val_auc: 0.6757\n",
      "Epoch 20/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0730 - auc: 0.6548 - val_loss: 5.0843 - val_auc: 0.6561\n",
      "Epoch 21/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0688 - auc: 0.6553 - val_loss: 5.1897 - val_auc: 0.6507\n",
      "Epoch 22/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4216 - auc: 0.6328 - val_loss: 5.1142 - val_auc: 0.6551\n",
      "Epoch 23/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0529 - auc: 0.6579 - val_loss: 5.5475 - val_auc: 0.6277\n",
      "Epoch 24/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4235 - auc: 0.6356 - val_loss: 5.1979 - val_auc: 0.6519\n",
      "Epoch 25/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.1449 - auc: 0.6545 - val_loss: 5.0295 - val_auc: 0.6620\n",
      "Epoch 26/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9544 - auc: 0.6653 - val_loss: 4.8227 - val_auc: 0.6740\n",
      "Epoch 27/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8621 - auc: 0.6681 - val_loss: 4.7195 - val_auc: 0.6768\n",
      "Epoch 28/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.9242 - auc: 0.6596 - val_loss: 4.9418 - val_auc: 0.6585\n",
      "Epoch 29/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.8494 - auc: 0.6629 - val_loss: 4.6269 - val_auc: 0.6752\n",
      "Epoch 30/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.5136 - auc: 0.6702 - val_loss: 5.0395 - val_auc: 0.6166\n",
      "Epoch 31/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.4707 - auc: 0.6295 - val_loss: 3.8472 - val_auc: 0.6570\n",
      "Epoch 32/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 3.5967 - auc: 0.6429 - val_loss: 3.8969 - val_auc: 0.6058\n",
      "Epoch 33/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 3.7823 - auc: 0.6060 - val_loss: 4.1484 - val_auc: 0.5817\n",
      "Epoch 34/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.6140 - auc: 0.6606 - val_loss: 4.6509 - val_auc: 0.6662\n",
      "Epoch 35/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.5794 - auc: 0.6706 - val_loss: 4.1073 - val_auc: 0.6920\n",
      "Epoch 36/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 3.6125 - auc: 0.6822 - val_loss: 2.7502 - val_auc: 0.6533\n",
      "Epoch 37/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 2.9793 - auc: 0.6387 - val_loss: 2.0049 - val_auc: 0.5703\n",
      "Epoch 38/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 3.2061 - auc: 0.5163 - val_loss: 9.5490 - val_auc: 0.3604\n",
      "Epoch 39/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 9.3084 - auc: 0.3741 - val_loss: 9.0810 - val_auc: 0.3909\n",
      "Epoch 40/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 8.8476 - auc: 0.4051 - val_loss: 8.6806 - val_auc: 0.4161\n",
      "Epoch 41/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 8.2961 - auc: 0.4417 - val_loss: 7.7867 - val_auc: 0.4750\n",
      "Epoch 42/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 7.4613 - auc: 0.4978 - val_loss: 7.0663 - val_auc: 0.5244\n",
      "Epoch 43/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 6.3696 - auc: 0.5707 - val_loss: 6.0895 - val_auc: 0.5888\n",
      "Epoch 44/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.8888 - auc: 0.5997 - val_loss: 5.4686 - val_auc: 0.6273\n",
      "Epoch 45/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.2915 - auc: 0.6381 - val_loss: 5.0038 - val_auc: 0.6559\n",
      "Epoch 46/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.3113 - auc: 0.6332 - val_loss: 4.9977 - val_auc: 0.6560\n",
      "Epoch 47/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9250 - auc: 0.6612 - val_loss: 4.7510 - val_auc: 0.6732\n",
      "Epoch 48/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0893 - auc: 0.6534 - val_loss: 4.9786 - val_auc: 0.6629\n",
      "Epoch 49/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0478 - auc: 0.6524 - val_loss: 4.8955 - val_auc: 0.6621\n",
      "Epoch 50/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.0635 - auc: 0.5886 - val_loss: 6.1720 - val_auc: 0.5868\n",
      "Epoch 51/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4806 - auc: 0.6297 - val_loss: 4.9131 - val_auc: 0.6677\n",
      "Epoch 52/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9030 - auc: 0.6676 - val_loss: 4.9746 - val_auc: 0.6634\n",
      "Epoch 53/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6030 - auc: 0.6260 - val_loss: 5.5659 - val_auc: 0.6331\n",
      "Epoch 54/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5753 - auc: 0.6313 - val_loss: 5.5158 - val_auc: 0.6360\n",
      "Epoch 55/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4863 - auc: 0.6369 - val_loss: 5.3338 - val_auc: 0.6473\n",
      "Epoch 56/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.3214 - auc: 0.6482 - val_loss: 5.1755 - val_auc: 0.6580\n",
      "Epoch 57/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.1751 - auc: 0.6582 - val_loss: 5.2821 - val_auc: 0.6531\n",
      "Epoch 58/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5256 - auc: 0.6370 - val_loss: 5.3862 - val_auc: 0.6468\n",
      "Epoch 59/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.9386 - auc: 0.6100 - val_loss: 6.3062 - val_auc: 0.5858\n",
      "Epoch 60/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.2165 - auc: 0.5926 - val_loss: 6.1161 - val_auc: 0.5997\n",
      "Epoch 61/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.0556 - auc: 0.6041 - val_loss: 5.9187 - val_auc: 0.6132\n",
      "Epoch 62/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.8705 - auc: 0.6167 - val_loss: 5.7190 - val_auc: 0.6266\n",
      "Epoch 63/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.7182 - auc: 0.6271 - val_loss: 5.5695 - val_auc: 0.6372\n",
      "Epoch 64/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.6200 - auc: 0.6339 - val_loss: 5.5110 - val_auc: 0.6408\n",
      "Epoch 65/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6242 - auc: 0.6332 - val_loss: 5.6771 - val_auc: 0.6294\n",
      "Epoch 66/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6414 - auc: 0.6320 - val_loss: 5.5304 - val_auc: 0.6396\n",
      "Epoch 67/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5461 - auc: 0.6385 - val_loss: 5.5011 - val_auc: 0.6417\n",
      "Epoch 68/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5386 - auc: 0.6392 - val_loss: 5.4599 - val_auc: 0.6443\n",
      "Epoch 69/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4768 - auc: 0.6434 - val_loss: 5.3977 - val_auc: 0.6482\n",
      "Epoch 70/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4025 - auc: 0.6484 - val_loss: 5.3216 - val_auc: 0.6535\n",
      "Epoch 71/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2836 - auc: 0.6559 - val_loss: 5.0796 - val_auc: 0.6690\n",
      "Epoch 72/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1653 - auc: 0.6637 - val_loss: 4.9969 - val_auc: 0.6742\n",
      "Epoch 73/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5316 - auc: 0.6394 - val_loss: 5.9506 - val_auc: 0.6111\n",
      "Epoch 74/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.8820 - auc: 0.6186 - val_loss: 5.7946 - val_auc: 0.6250\n",
      "Epoch 75/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.8111 - auc: 0.6242 - val_loss: 5.8143 - val_auc: 0.6240\n",
      "Epoch 76/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.8562 - auc: 0.6216 - val_loss: 5.8374 - val_auc: 0.6231\n",
      "Epoch 77/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.8586 - auc: 0.6218 - val_loss: 5.8191 - val_auc: 0.6239\n",
      "Epoch 78/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.8353 - auc: 0.6233 - val_loss: 5.8232 - val_auc: 0.6241\n",
      "Epoch 79/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.7725 - auc: 0.6275 - val_loss: 5.7322 - val_auc: 0.6297\n",
      "Epoch 80/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6825 - auc: 0.6333 - val_loss: 5.6254 - val_auc: 0.6367\n",
      "Epoch 81/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.5736 - auc: 0.6406 - val_loss: 5.4738 - val_auc: 0.6470\n",
      "Epoch 82/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.4842 - auc: 0.6465 - val_loss: 5.4303 - val_auc: 0.6498\n",
      "Epoch 83/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.3983 - auc: 0.6521 - val_loss: 5.2883 - val_auc: 0.6592\n",
      "Epoch 84/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2905 - auc: 0.6591 - val_loss: 5.1772 - val_auc: 0.6668\n",
      "Epoch 85/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.3112 - auc: 0.6578 - val_loss: 5.3565 - val_auc: 0.6548\n",
      "Epoch 86/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.3425 - auc: 0.6561 - val_loss: 5.2107 - val_auc: 0.6646\n",
      "Epoch 87/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2645 - auc: 0.6616 - val_loss: 5.1871 - val_auc: 0.6667\n",
      "Epoch 88/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2484 - auc: 0.6628 - val_loss: 5.1192 - val_auc: 0.6711\n",
      "Epoch 89/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1839 - auc: 0.6671 - val_loss: 5.0826 - val_auc: 0.6734\n",
      "Epoch 90/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1120 - auc: 0.6721 - val_loss: 5.0326 - val_auc: 0.6774\n",
      "Epoch 91/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1445 - auc: 0.6700 - val_loss: 5.0560 - val_auc: 0.6755\n",
      "Epoch 92/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1037 - auc: 0.6727 - val_loss: 4.9932 - val_auc: 0.6798\n",
      "Epoch 93/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.0112 - auc: 0.6787 - val_loss: 4.9026 - val_auc: 0.6856\n",
      "Epoch 94/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2376 - auc: 0.6647 - val_loss: 5.6093 - val_auc: 0.6424\n",
      "Epoch 95/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5984 - auc: 0.6429 - val_loss: 5.4796 - val_auc: 0.6508\n",
      "Epoch 96/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6570 - auc: 0.6393 - val_loss: 5.7836 - val_auc: 0.6309\n",
      "Epoch 97/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.7298 - auc: 0.6345 - val_loss: 5.6785 - val_auc: 0.6379\n",
      "Epoch 98/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.7172 - auc: 0.6355 - val_loss: 5.7647 - val_auc: 0.6317\n",
      "Epoch 99/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.7624 - auc: 0.6325 - val_loss: 5.7559 - val_auc: 0.6332\n",
      "Epoch 100/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6919 - auc: 0.6373 - val_loss: 5.6463 - val_auc: 0.6402\n",
      "Epoch 101/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5951 - auc: 0.6438 - val_loss: 5.5219 - val_auc: 0.6484\n",
      "Epoch 102/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4603 - auc: 0.6526 - val_loss: 5.2881 - val_auc: 0.6634\n",
      "Epoch 103/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2882 - auc: 0.6640 - val_loss: 5.1618 - val_auc: 0.6719\n",
      "Epoch 104/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.3254 - auc: 0.6616 - val_loss: 5.2242 - val_auc: 0.6680\n",
      "Epoch 105/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2464 - auc: 0.6669 - val_loss: 5.1669 - val_auc: 0.6723\n",
      "Epoch 106/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2126 - auc: 0.6696 - val_loss: 5.0728 - val_auc: 0.6783\n",
      "Epoch 107/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1202 - auc: 0.6751 - val_loss: 4.9978 - val_auc: 0.6827\n",
      "Epoch 108/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0395 - auc: 0.6801 - val_loss: 4.9551 - val_auc: 0.6857\n",
      "Epoch 109/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9699 - auc: 0.6849 - val_loss: 4.8663 - val_auc: 0.6917\n",
      "Epoch 110/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.8955 - auc: 0.6897 - val_loss: 4.7653 - val_auc: 0.6979\n",
      "Epoch 111/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0051 - auc: 0.6828 - val_loss: 4.9189 - val_auc: 0.6882\n",
      "Epoch 112/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9632 - auc: 0.6854 - val_loss: 4.8678 - val_auc: 0.6912\n",
      "Epoch 113/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9396 - auc: 0.6868 - val_loss: 4.8352 - val_auc: 0.6933\n",
      "Epoch 114/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8922 - auc: 0.6898 - val_loss: 4.7597 - val_auc: 0.6979\n",
      "Epoch 115/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8100 - auc: 0.6949 - val_loss: 4.6926 - val_auc: 0.7018\n",
      "Epoch 116/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.7751 - auc: 0.6973 - val_loss: 4.6623 - val_auc: 0.7046\n",
      "Epoch 117/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.7404 - auc: 0.6998 - val_loss: 4.6192 - val_auc: 0.7075\n",
      "Epoch 118/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.7430 - auc: 0.6996 - val_loss: 4.6013 - val_auc: 0.7084\n",
      "Epoch 119/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7690 - auc: 0.6977 - val_loss: 4.9426 - val_auc: 0.6863\n",
      "Epoch 120/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.8872 - auc: 0.6902 - val_loss: 4.7441 - val_auc: 0.6992\n",
      "Epoch 121/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8144 - auc: 0.6949 - val_loss: 4.7026 - val_auc: 0.7024\n",
      "Epoch 122/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.7869 - auc: 0.6967 - val_loss: 4.7103 - val_auc: 0.7016\n",
      "Epoch 123/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.7871 - auc: 0.6965 - val_loss: 4.7701 - val_auc: 0.6972\n",
      "Epoch 124/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8687 - auc: 0.6906 - val_loss: 4.8478 - val_auc: 0.6918\n",
      "Epoch 125/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.8488 - auc: 0.6918 - val_loss: 4.7746 - val_auc: 0.6963\n",
      "Epoch 126/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.8036 - auc: 0.6949 - val_loss: 4.7355 - val_auc: 0.6995\n",
      "Epoch 127/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7661 - auc: 0.6975 - val_loss: 4.6474 - val_auc: 0.7050\n",
      "Epoch 128/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7040 - auc: 0.7015 - val_loss: 4.5877 - val_auc: 0.7088\n",
      "Epoch 129/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7079 - auc: 0.7013 - val_loss: 4.6910 - val_auc: 0.7028\n",
      "Epoch 130/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.7066 - auc: 0.7014 - val_loss: 4.6554 - val_auc: 0.7047\n",
      "Epoch 131/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7219 - auc: 0.7008 - val_loss: 4.6096 - val_auc: 0.7079\n",
      "Epoch 132/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7038 - auc: 0.7021 - val_loss: 4.6037 - val_auc: 0.7081\n",
      "Epoch 133/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7184 - auc: 0.7010 - val_loss: 4.6717 - val_auc: 0.7034\n",
      "Epoch 134/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.7044 - auc: 0.7016 - val_loss: 4.5744 - val_auc: 0.7095\n",
      "Epoch 135/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.6535 - auc: 0.7046 - val_loss: 4.5387 - val_auc: 0.7116\n",
      "Epoch 136/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.6325 - auc: 0.7058 - val_loss: 4.4974 - val_auc: 0.7144\n",
      "Epoch 137/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.5990 - auc: 0.7079 - val_loss: 4.4459 - val_auc: 0.7176\n",
      "validation quality metric 0.5704131728302209\n",
      "clearing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                             | 1/10 [06:30<58:36, 390.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2a:00.0, compute capability: 8.6\n",
      "\n",
      "Epoch 1/600\n",
      "90/90 [==============================] - 3s 29ms/step - loss: 5.6707 - auc: 0.6022 - val_loss: 4.8978 - val_auc: 0.6150\n",
      "Epoch 2/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.5823 - auc: 0.5597 - val_loss: 6.7265 - val_auc: 0.5314\n",
      "Epoch 3/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.3095 - auc: 0.5764 - val_loss: 7.2688 - val_auc: 0.5091\n",
      "Epoch 4/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 7.1147 - auc: 0.5262 - val_loss: 6.2424 - val_auc: 0.5887\n",
      "Epoch 5/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.5104 - auc: 0.5702 - val_loss: 6.3174 - val_auc: 0.5822\n",
      "Epoch 6/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.1094 - auc: 0.5953 - val_loss: 6.2715 - val_auc: 0.5850\n",
      "Epoch 7/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.2114 - auc: 0.5882 - val_loss: 5.9260 - val_auc: 0.6090\n",
      "Epoch 8/600\n",
      "90/90 [==============================] - 2s 28ms/step - loss: 6.1579 - auc: 0.5933 - val_loss: 5.9269 - val_auc: 0.6109\n",
      "Epoch 9/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.9018 - auc: 0.6112 - val_loss: 5.7839 - val_auc: 0.6212\n",
      "Epoch 10/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.7152 - auc: 0.6242 - val_loss: 5.4941 - val_auc: 0.6387\n",
      "Epoch 11/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4866 - auc: 0.6388 - val_loss: 5.2685 - val_auc: 0.6532\n",
      "Epoch 12/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2842 - auc: 0.6521 - val_loss: 5.0696 - val_auc: 0.6659\n",
      "Epoch 13/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0879 - auc: 0.6641 - val_loss: 5.0337 - val_auc: 0.6663\n",
      "Epoch 14/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0863 - auc: 0.6633 - val_loss: 5.0842 - val_auc: 0.6645\n",
      "Epoch 15/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.3379 - auc: 0.6481 - val_loss: 5.4801 - val_auc: 0.6405\n",
      "Epoch 16/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.5611 - auc: 0.6344 - val_loss: 5.5561 - val_auc: 0.6358\n",
      "Epoch 17/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4442 - auc: 0.6425 - val_loss: 5.2971 - val_auc: 0.6523\n",
      "Epoch 18/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.3549 - auc: 0.6488 - val_loss: 5.1883 - val_auc: 0.6594\n",
      "Epoch 19/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2935 - auc: 0.6524 - val_loss: 5.5736 - val_auc: 0.6323\n",
      "Epoch 20/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6163 - auc: 0.6284 - val_loss: 5.4793 - val_auc: 0.6384\n",
      "Epoch 21/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4359 - auc: 0.6407 - val_loss: 5.4922 - val_auc: 0.6355\n",
      "Epoch 22/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5338 - auc: 0.6351 - val_loss: 5.5410 - val_auc: 0.6395\n",
      "Epoch 23/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.5454 - auc: 0.6375 - val_loss: 5.4365 - val_auc: 0.6457\n",
      "Epoch 24/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.7217 - auc: 0.6263 - val_loss: 6.0963 - val_auc: 0.6027\n",
      "Epoch 25/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 6.3004 - auc: 0.5882 - val_loss: 6.5438 - val_auc: 0.5731\n",
      "Epoch 26/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.3007 - auc: 0.5889 - val_loss: 6.1202 - val_auc: 0.6019\n",
      "Epoch 27/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 6.0736 - auc: 0.6044 - val_loss: 5.9497 - val_auc: 0.6124\n",
      "Epoch 28/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.8512 - auc: 0.6198 - val_loss: 5.6745 - val_auc: 0.6323\n",
      "Epoch 29/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.6944 - auc: 0.6307 - val_loss: 5.5319 - val_auc: 0.6410\n",
      "Epoch 30/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.5927 - auc: 0.6371 - val_loss: 5.4359 - val_auc: 0.6478\n",
      "Epoch 31/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.4621 - auc: 0.6457 - val_loss: 5.3582 - val_auc: 0.6525\n",
      "Epoch 32/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4744 - auc: 0.6448 - val_loss: 5.3663 - val_auc: 0.6518\n",
      "Epoch 33/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6156 - auc: 0.6353 - val_loss: 5.6611 - val_auc: 0.6322\n",
      "Epoch 34/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.6271 - auc: 0.6347 - val_loss: 5.5031 - val_auc: 0.6435\n",
      "Epoch 35/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.5351 - auc: 0.6412 - val_loss: 5.4531 - val_auc: 0.6464\n",
      "Epoch 36/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4249 - auc: 0.6484 - val_loss: 5.2498 - val_auc: 0.6599\n",
      "Epoch 37/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2867 - auc: 0.6578 - val_loss: 5.0819 - val_auc: 0.6712\n",
      "Epoch 38/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1754 - auc: 0.6658 - val_loss: 4.9949 - val_auc: 0.6785\n",
      "Epoch 39/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.0790 - auc: 0.6727 - val_loss: 4.9034 - val_auc: 0.6844\n",
      "Epoch 40/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.0495 - auc: 0.6746 - val_loss: 4.8625 - val_auc: 0.6870\n",
      "Epoch 41/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9805 - auc: 0.6791 - val_loss: 4.8114 - val_auc: 0.6903\n",
      "Epoch 42/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9454 - auc: 0.6813 - val_loss: 4.7615 - val_auc: 0.6934\n",
      "Epoch 43/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.9274 - auc: 0.6828 - val_loss: 4.7643 - val_auc: 0.6936\n",
      "Epoch 44/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.8913 - auc: 0.6852 - val_loss: 4.7039 - val_auc: 0.6975\n",
      "Epoch 45/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 4.8391 - auc: 0.6883 - val_loss: 4.6802 - val_auc: 0.6989\n",
      "Epoch 46/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 4.8748 - auc: 0.6859 - val_loss: 4.8660 - val_auc: 0.6864\n",
      "Epoch 47/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.0039 - auc: 0.6773 - val_loss: 4.9247 - val_auc: 0.6819\n",
      "Epoch 48/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1725 - auc: 0.6662 - val_loss: 5.1629 - val_auc: 0.6666\n",
      "Epoch 49/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1993 - auc: 0.6645 - val_loss: 5.0109 - val_auc: 0.6770\n",
      "Epoch 50/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.1609 - auc: 0.6674 - val_loss: 5.1334 - val_auc: 0.6699\n",
      "Epoch 51/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2447 - auc: 0.6623 - val_loss: 5.2019 - val_auc: 0.6650\n",
      "Epoch 52/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.2009 - auc: 0.6649 - val_loss: 5.0103 - val_auc: 0.6772\n",
      "Epoch 53/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.1148 - auc: 0.6706 - val_loss: 4.9247 - val_auc: 0.6832\n",
      "Epoch 54/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.7423 - auc: 0.6297 - val_loss: 5.8117 - val_auc: 0.6248\n",
      "Epoch 55/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.8447 - auc: 0.6232 - val_loss: 5.9089 - val_auc: 0.6192\n",
      "Epoch 56/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.9079 - auc: 0.6188 - val_loss: 5.8387 - val_auc: 0.6233\n",
      "Epoch 57/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.0568 - auc: 0.6096 - val_loss: 6.3149 - val_auc: 0.5959\n",
      "Epoch 58/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 6.3763 - auc: 0.5926 - val_loss: 6.2458 - val_auc: 0.6006\n",
      "Epoch 59/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 6.1900 - auc: 0.6047 - val_loss: 6.0840 - val_auc: 0.6111\n",
      "Epoch 60/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 6.0534 - auc: 0.6136 - val_loss: 5.8004 - val_auc: 0.6302\n",
      "Epoch 61/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.8216 - auc: 0.6291 - val_loss: 5.7942 - val_auc: 0.6304\n",
      "Epoch 62/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.8152 - auc: 0.6296 - val_loss: 5.6965 - val_auc: 0.6370\n",
      "Epoch 63/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.7025 - auc: 0.6369 - val_loss: 5.5630 - val_auc: 0.6460\n",
      "Epoch 64/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.6113 - auc: 0.6430 - val_loss: 5.4992 - val_auc: 0.6501\n",
      "Epoch 65/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4837 - auc: 0.6522 - val_loss: 5.3029 - val_auc: 0.6641\n",
      "Epoch 66/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.4098 - auc: 0.6572 - val_loss: 5.2536 - val_auc: 0.6670\n",
      "Epoch 67/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.3310 - auc: 0.6618 - val_loss: 5.1770 - val_auc: 0.6713\n",
      "Epoch 68/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.2836 - auc: 0.6642 - val_loss: 5.1907 - val_auc: 0.6698\n",
      "Epoch 69/600\n",
      "90/90 [==============================] - 2s 26ms/step - loss: 5.3153 - auc: 0.6619 - val_loss: 5.2120 - val_auc: 0.6687\n",
      "Epoch 70/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 5.4206 - auc: 0.6555 - val_loss: 5.4430 - val_auc: 0.6545\n",
      "Epoch 71/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 6.0806 - auc: 0.6127 - val_loss: 6.1741 - val_auc: 0.6070\n",
      "Epoch 72/600\n",
      "90/90 [==============================] - 2s 27ms/step - loss: 6.1055 - auc: 0.6111 - val_loss: 5.9414 - val_auc: 0.6219\n",
      "Epoch 73/600\n",
      "36/90 [===========>..................] - ETA: 1s - loss: 5.9660 - auc: 0.6209"
     ]
    }
   ],
   "source": [
    "out_of_fold_predictions = np.zeros((X.shape[0], len(model_types)))\n",
    "meta_features = np.zeros((test.shape[0], len(model_types)))\n",
    "y_pred = []\n",
    "for i, model_type in enumerate(model_types):\n",
    "    print(\"starting \" + model_type[\"name\"])\n",
    "    model_fname = \"./models/\"+model_type[\"name\"] +\"_\"+str(splits[\"hash\"])+\".npy\"\n",
    "    if os.path.exists(model_fname):\n",
    "        print(\"loading model\")\n",
    "        model_data_arr = np.load(model_fname, allow_pickle=True)\n",
    "        model_data = model_data_arr[0]\n",
    "    else:\n",
    "        print(\"fitting model\")\n",
    "        mse = []\n",
    "        meta_features_folds = np.zeros((test.shape[0], spl))\n",
    "        out_of_fold_predictions_folds = np.zeros((X.shape[0], ))\n",
    "        for i_fold, split in enumerate(tqdm(splits[\"list\"])):\n",
    "            train_idx = split[\"train\"]\n",
    "            valid_idx = split[\"valid\"]\n",
    "            Xt = X.iloc[train_idx, :]\n",
    "            yt = y.iloc[train_idx]\n",
    "            Xv = X.iloc[valid_idx, :]\n",
    "            yv = y.iloc[valid_idx]\n",
    "            model = model_type['create']()\n",
    "            model_type['fit'](model, Xt, yt, Xv, yv)\n",
    "            y_pred = model_type['predict'](model, Xv)\n",
    "            out_of_fold_predictions_folds[valid_idx] = y_pred\n",
    "            valid_mse = roc_auc_score(y_true = yv, y_score = y_pred)\n",
    "            mse.append(valid_mse)\n",
    "            meta_features_folds[:, i_fold] = model_type['predict'](model, test)\n",
    "            print(\"validation quality metric\", valid_mse)\n",
    "            if \"clear\" in model_type:\n",
    "                print(\"clearing\")\n",
    "                model_type[\"clear\"](model)\n",
    "            del model\n",
    "            gc.collect()\n",
    "        if \"shutdown\" in model_type:\n",
    "            print(\"shutting down\")\n",
    "            model_type[\"shutdown\"]()\n",
    "        meta_features_folds = meta_features_folds.mean(axis=1)\n",
    "        model_data={}\n",
    "        model_data['meta_features_folds'] = meta_features_folds\n",
    "        model_data['out_of_fold_predictions_folds'] = out_of_fold_predictions_folds\n",
    "        model_data['mse'] = mse\n",
    "        model_data_arr = [model_data]\n",
    "        np.save(model_fname, model_data_arr)\n",
    "    meta_features[:, i] = model_data['meta_features_folds']\n",
    "    out_of_fold_predictions[:, i] = model_data['out_of_fold_predictions_folds']\n",
    "    mse = model_data['mse']\n",
    "    print(model_type[\"name\"], i, mse, \" max \", max(mse), \" min \", min(mse),\" avg \", sum(mse)/len(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c254c-6526-46d6-9198-eadb00794d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01db3ef-09d7-4d4d-8b78-fac58e853bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate ridge\n",
    "spl = 10\n",
    "kf = StratifiedKFold(n_splits=spl, shuffle=True)\n",
    "test_pred_total = np.zeros(len(rawtest))\n",
    "mse = []\n",
    "for train_idx, valid_idx in tqdm(kf.split(out_of_fold_predictions,y.round())):\n",
    "        model = meta_model\n",
    "        Xt = out_of_fold_predictions[train_idx, :]\n",
    "        yt = y.iloc[train_idx]\n",
    "        Xv = out_of_fold_predictions[valid_idx, :]\n",
    "        yv = y.iloc[valid_idx]\n",
    "        model.fit(Xt, yt)\n",
    "        y_pred = model.predict(Xv)\n",
    "        valid_mse = roc_auc_score(y_true = y.iloc[valid_idx], y_score = y_pred)\n",
    "        print(\"valid qlty \", valid_mse)\n",
    "        mse.append(valid_mse)\n",
    "        test_pred = model.predict(meta_features) / spl\n",
    "        test_pred_total += test_pred\n",
    "print(\"valid qlty\", mse, \" min \", min(mse), \" max \", max(mse), \" avg \", sum(mse)/len(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d78a8-1d76-4a9e-8f5a-163dac6b6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': rawtest.index,\n",
    "                       'target': test_pred_total})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89bba3-ad9e-4cb1-b5e7-08254f12e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8af79d-27be-4ac7-8020-8c3f0f7c09c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b42b52-0159-4a57-8bb3-4216b22f66ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
